import io
import time
import threading
import librosa
import numpy as np
import soundfile as sf
import torch
from loguru import logger
from data.fish_speech.fish_speech.inference_engine import TTSInferenceEngine
from data.fish_speech.fish_speech.models.dac.inference import load_model as load_decoder_model
from data.fish_speech.fish_speech.models.text2semantic.inference import launch_thread_safe_queue
from data.fish_speech.fish_speech.utils.schema import ServeTTSRequest, ServeReferenceAudio
from data.fish_speech.tools.server.inference import inference_wrapper as inference


class TTSGenerator:
    # æ·»åŠ ç±»å˜é‡ç”¨äºç¼“å­˜å®ä¾‹
    _instances = {}
    _lock = threading.Lock()

    def __new__(
            cls,
            llama_checkpoint_path: str,
            decoder_checkpoint_path: str,
            decoder_config_name: str = "modded_dac_vq",
            device: str = "auto",
            half: bool = False,
            compile_model: bool = True,
            max_text_length: int = 200
    ):
        # åˆ›å»ºå”¯ä¸€çš„ç¼“å­˜é”®
        cache_key = (llama_checkpoint_path, decoder_checkpoint_path, decoder_config_name,
                     device, half, compile_model, max_text_length)

        # åŒæ£€é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        if cache_key not in cls._instances:
            with cls._lock:
                if cache_key not in cls._instances:
                    logger.info(f"ğŸš€ åˆ›å»ºæ–°çš„TTSç”Ÿæˆå™¨å®ä¾‹ | ç¼“å­˜é”®: {cache_key[:2]}")
                    instance = super().__new__(cls)
                    instance._initialized = False
                    cls._instances[cache_key] = instance
        return cls._instances[cache_key]

    def __init__(
            self,
            llama_checkpoint_path: str,
            decoder_checkpoint_path: str,
            decoder_config_name: str = "modded_dac_vq",
            device: str = "auto",
            half: bool = False,
            compile_model: bool = True,
            max_text_length: int = 200
    ):
        # é˜²æ­¢é‡å¤åˆå§‹åŒ–
        if self._initialized:
            return

        self._initialized = True
        self.cache_key = (llama_checkpoint_path, decoder_checkpoint_path)

        # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        self.device = device
        self.half = half
        self.compile_model = compile_model
        self.max_text_length = max_text_length
        self.precision = torch.half if half else torch.bfloat16

        logger.info(f"ğŸš€ åˆå§‹åŒ–TTSç”Ÿæˆå™¨ | è®¾å¤‡: {device} | åŠç²¾åº¦: {half} | ç¼–è¯‘æ¨¡å‹: {compile_model}")

        # åŠ è½½LLAMAæ¨¡å‹ (æ–‡æœ¬è½¬è¯­ä¹‰token)
        logger.info("â³ åŠ è½½LLAMAæ¨¡å‹...")
        self.llama_queue = launch_thread_safe_queue(
            checkpoint_path=llama_checkpoint_path,
            device=self.device,
            precision=self.precision,
            compile=self.compile_model,
        )

        # åŠ è½½è§£ç å™¨æ¨¡å‹ (è¯­ä¹‰tokenè½¬éŸ³é¢‘)
        logger.info("â³ åŠ è½½è§£ç å™¨æ¨¡å‹...")
        self.decoder_model = load_decoder_model(
            config_name=decoder_config_name,
            checkpoint_path=decoder_checkpoint_path,
            device=self.device,
        )

        # åˆ›å»ºTTSæ¨ç†å¼•æ“
        self.tts_engine = TTSInferenceEngine(
            llama_queue=self.llama_queue,
            decoder_model=self.decoder_model,
            precision=self.precision,
            compile=self.compile_model,
        )

        # é¢„çƒ­æ¨¡å‹
        self.warm_up()
        logger.success(f"âœ… TTSç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ | ç¼“å­˜é”®: {self.cache_key[:2]}")

    def warm_up(self):
        """é¢„çƒ­æ¨¡å‹ä»¥ç¡®ä¿é¦–æ¬¡æ¨ç†é€Ÿåº¦"""
        logger.info("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        request = ServeTTSRequest(
            text="æ¬¢è¿ä½¿ç”¨è¯­éŸ³åˆæˆæœåŠ¡",
            max_new_tokens=0,
            chunk_length=200,
            top_p=0.7,
            repetition_penalty=1.2,
            temperature=0.7,
            format="wav",
            references=[]
        )
        # æ‰§è¡Œä¸€æ¬¡æ¨ç†ä½†ä¸ä¿å­˜ç»“æœ
        list(inference(request, self.tts_engine))
        logger.info("ğŸ”¥ æ¨¡å‹é¢„çƒ­å®Œæˆ")


    def generate_speech(
            self,
            text: str,
            seed: int,
            speed_factor: float = 1.0,
            output_format: str = "wav",
            top_p: float = 0.7,
            temperature: float = 0.7,
            repetition_penalty: float = 1.2,
            references: list[dict] = None,  # æ·»åŠ å‚è€ƒéŸ³é¢‘æ”¯æŒ
            max_new_tokens: int = 1024,  # æ·»åŠ max_new_tokenså‚æ•°
            chunk_length: int = 200,  # æ·»åŠ chunk_lengthå‚æ•°
    ) -> bytes:
        """
        ç”Ÿæˆè¯­éŸ³

        :param text: è¦åˆæˆçš„æ–‡æœ¬
        :param seed: éšæœºç§å­
        :param speed_factor: è¯­é€Ÿå› å­ (1.0ä¸ºæ­£å¸¸è¯­é€Ÿ)
        :param output_format: è¾“å‡ºæ ¼å¼ (wav/pcm/mp3)
        :param top_p: é‡‡æ ·æ¦‚ç‡é˜ˆå€¼ (æ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§)
        :param temperature: æ¸©åº¦å‚æ•° (æ§åˆ¶éšæœºæ€§)
        :param repetition_penalty: é‡å¤æƒ©ç½šå› å­ (é¿å…é‡å¤)
        :param references: å‚è€ƒéŸ³é¢‘åˆ—è¡¨ [{"audio": base64, "text": str}]
        :param max_new_tokens: æœ€å¤§æ–°tokenæ•°
        :param chunk_length: åˆ†å—é•¿åº¦
        :return: éŸ³é¢‘äºŒè¿›åˆ¶æ•°æ®
        """
        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if self.max_text_length > 0 and len(text) > self.max_text_length:
            raise ValueError(f"æ–‡æœ¬è¿‡é•¿ (æœ€å¤§é•¿åº¦: {self.max_text_length})")

        # è½¬æ¢å‚è€ƒéŸ³é¢‘æ ¼å¼
        serve_references = []
        if references:
            for ref in references:
                serve_references.append(
                    ServeReferenceAudio(audio=ref["audio"], text=ref["text"])
                )

        # åˆ›å»ºTTSè¯·æ±‚
        request = ServeTTSRequest(
            text=text,
            format=output_format,
            seed=seed,
            max_new_tokens=max_new_tokens,
            chunk_length=chunk_length,
            top_p=top_p,
            temperature=temperature,
            repetition_penalty=repetition_penalty,
            streaming=False,
            references=serve_references,  # æ·»åŠ å‚è€ƒéŸ³é¢‘
        )

        # æ‰§è¡ŒTTSæ¨ç†
        logger.info(f"ğŸ”Š åˆæˆè¯­éŸ³ | é•¿åº¦: {len(text)}å­—ç¬¦ | è¯­é€Ÿ: {speed_factor}x")
        start_time = time.time()

        # è·å–ç”Ÿæˆçš„éŸ³é¢‘æ•°æ®
        fake_audios = next(inference(request, self.tts_engine))

        # åº”ç”¨è¯­é€Ÿè°ƒæ•´
        if speed_factor != 1.0:
            fake_audios = self.adjust_speech_speed(fake_audios, speed_factor)

        # è½¬æ¢ä¸ºéŸ³é¢‘æ–‡ä»¶æ ¼å¼
        audio_data = self.save_audio(fake_audios, output_format)

        logger.info(f"â±ï¸ åˆæˆå®Œæˆ | è€—æ—¶: {time.time() - start_time:.2f}ç§’")
        return audio_data

    def adjust_speech_speed(self, audio: np.ndarray, factor: float) -> np.ndarray:
        """
        è°ƒæ•´è¯­éŸ³é€Ÿåº¦

        :param audio: åŸå§‹éŸ³é¢‘æ•°æ®
        :param factor: é€Ÿåº¦å› å­ (>1åŠ é€Ÿ, <1å‡é€Ÿ)
        :return: è°ƒæ•´åçš„éŸ³é¢‘
        """
        if audio.dtype != np.float32:
            audio = audio.astype(np.float32)

        # å•å£°é“å¤„ç†
        if audio.ndim == 1:
            return librosa.effects.time_stretch(audio, rate=factor)

        # å¤šå£°é“å¤„ç†
        processed_channels = []
        for channel in range(audio.shape[1]):
            processed_channels.append(
                librosa.effects.time_stretch(audio[:, channel], rate=factor)
            )

        # å¯¹é½æ‰€æœ‰å£°é“é•¿åº¦
        min_length = min(len(ch) for ch in processed_channels)
        return np.stack([ch[:min_length] for ch in processed_channels], axis=1)

    def save_audio(self, audio: np.ndarray, format: str) -> bytes:
        """
        å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼

        :param audio: éŸ³é¢‘numpyæ•°ç»„
        :param format: éŸ³é¢‘æ ¼å¼ (wav/pcm/mp3)
        :return: äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
        """
        buffer = io.BytesIO()
        sf.write(buffer, audio, self.decoder_model.sample_rate, format=format)
        buffer.seek(0)
        return buffer.read()


# å…¨å±€ç¼“å­˜å­—å…¸
_TTS_CACHE = {}
_TTS_CACHE_LOCK = threading.Lock()


def fish_voice(text, output_format, references,seed,speed_factor,top_p,temperature,repetition_penalty):
    llama_model_path = "D:/develop/project/fish-speech/tools/checkpoints/openaudio-s1-mini"
    decoder_model_path = "D:/develop/project/fish-speech/tools/checkpoints/openaudio-s1-mini/codec.pth"
    # ä½¿ç”¨ç¼“å­˜é”®
    cache_key = (llama_model_path, decoder_model_path)

    # æ£€æŸ¥ç¼“å­˜
    with _TTS_CACHE_LOCK:
        if cache_key in _TTS_CACHE:
            logger.info(f"â™»ï¸ å¤ç”¨å·²åŠ è½½çš„TTSæ¨¡å‹ | ç¼“å­˜é”®: {cache_key}")
            tts = _TTS_CACHE[cache_key]
        else:
            logger.info(f"ğŸ†• åˆå§‹åŒ–TTSæ¨¡å‹ | ç¼“å­˜é”®: {cache_key}")
            start_time = time.time()
            tts = TTSGenerator(
                llama_checkpoint_path=llama_model_path,
                decoder_checkpoint_path=decoder_model_path,
                device="auto",
                half=False,
                compile_model=False,
                max_text_length=200
            )
            logger.info(f"ğŸ•’ åˆå§‹åŒ–è€—æ—¶: {time.time() - start_time:.2f}ç§’")
            _TTS_CACHE[cache_key] = tts

    # ç”Ÿæˆè¯­éŸ³
    audio_data = tts.generate_speech(
        text=text,
        seed=seed,
        speed_factor=speed_factor,
        output_format=output_format,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        references=references,
        max_new_tokens=1024,
        chunk_length=200
    )
    return audio_data



# æ¸…ç†ç¼“å­˜çš„å‡½æ•°
def clear_tts_cache():
    global _TTS_CACHE
    with _TTS_CACHE_LOCK:
        logger.info("ğŸ§¹ æ¸…ç†TTSæ¨¡å‹ç¼“å­˜")
        _TTS_CACHE = {}
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")


if __name__ == '__main__':
    from util import file_util
    import os
    import uuid
    import config
    # clear_tts_cache()
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # ===== é…ç½®å‚æ•° =====
    # ç”Ÿæˆå‚æ•°
    text = "æ¬¢è¿ä½¿ç”¨é«˜è´¨é‡è¯­éŸ³åˆæˆæœåŠ¡ï¼Œæœ¬æœåŠ¡æä¾›è‡ªç„¶æµç•…çš„è¯­éŸ³è¾“å‡º"
    output_file = "output.wav"

    # references  = []
    references = [{
        "audio": file_util.audio_to_base64("E:/aupi/sound/hutao/39.èƒ¡æ¡ƒçš„çˆ±å¥½â€¦_å¤©æ¸…æµ·é˜”ï¼Œçš“æœˆå‡Œç©ºï¼Œæ­¤æƒ…æ­¤æ™¯ï¼Œæ­£é€‚åˆä½œè¯—ä¸€é¦–ã€‚.mp3"),
        # å®é™…ä½¿ç”¨base64ç¼–ç çš„éŸ³é¢‘å­—ç¬¦ä¸²
        "text": "å¤©æ¸…æµ·é˜”ï¼Œçš“æœˆå‡Œç©ºï¼Œæ­¤æƒ…æ­¤æ™¯ï¼Œæ­£é€‚åˆä½œè¯—ä¸€é¦–ã€‚"
    }]
    seed = 876888
    speed_factor = 1.0
    top_p = 0.8
    temperature = 0.7
    repetition_penalty=1.1
    output_format = "wav"
    audio_data = fish_voice(text, output_format, references, seed, speed_factor, top_p,
                                       temperature, repetition_penalty)

    filename = f"{uuid.uuid4().hex}.{output_format}"
    file_path = os.path.join(config.UPLOAD_DIR, filename)
    # ä¿å­˜ç»“æœ
    with open(file_path, "wb") as f:
        f.write(audio_data)